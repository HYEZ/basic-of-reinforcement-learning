{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 러닝\n",
    "- 모든 상태(행)과 액션(열)에 대한 테이블\n",
    "    - 각 셀은 어떤 상태에서 액션을 취했을 때 그것이 얼마나 좋은지 수치화한 값 => 이걸 학습\n",
    "- openAI 얼어붙은 강 환경\n",
    "    - 16가지 상태, 4가지 액션\n",
    "    - 16x4의 Q-테이블\n",
    "        - 첨엔 0으로 다 초기화, 다양한 액션에 대해 얻게 되는 보상을 관찰하면서 테이블 업데이트(벨먼 방정식으로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 테이블 학습 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테이블을 모두 0으로 초기화함\n",
    "Q = np.zeros([env.observation_space.n, # state 수\n",
    "              env.action_space.n]) # action 수\n",
    "\n",
    "# 파라미터 설정\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "\n",
    "# 보상의 총계를 담을 리스트 생성\n",
    "rList = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # 환경을 리셋하고 새로운 관찰 얻는다.\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    \n",
    "    # Q 테이블 학습 알고리즘\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        # Q 테이블로부터 (노이즈와 함께) 그리디하게 액션 선택\n",
    "        a = np.argmax(Q[s,:] +\\\n",
    "                      np.random.randn(1,env.action_space.n)*\\\n",
    "                      (1./(i+1)))\n",
    "        \n",
    "        # 환경으로부터 새로운 상태와 보상을 얻는다.\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        \n",
    "        # 새로운 지식을 통해 Q 테이블 업데이트함\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        \n",
    "        rAll += r\n",
    "        s = s1\n",
    "        \n",
    "        if d == True:\n",
    "            break\n",
    "\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.482\n"
     ]
    }
   ],
   "source": [
    "# 성공한 에피소드 비율\n",
    "print(\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[2.03701379e-01 5.39100721e-03 7.85564445e-03 4.16704718e-03]\n",
      " [4.34403627e-03 1.98092338e-04 2.12282702e-03 1.62698981e-01]\n",
      " [2.05862532e-03 1.42335441e-03 1.00479313e-03 1.13143596e-01]\n",
      " [2.11991172e-05 3.08814407e-04 1.07100235e-03 1.03969291e-01]\n",
      " [2.27342502e-01 1.41319297e-04 8.74305287e-04 6.38522870e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.49517237e-02 8.33473035e-06 2.01013481e-04 3.91730874e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [9.67208247e-04 2.35286181e-03 1.85174664e-04 4.29715531e-01]\n",
      " [1.09012356e-03 2.13229829e-01 3.38498294e-05 1.08087886e-05]\n",
      " [7.58677695e-01 9.97701051e-05 9.34183652e-05 1.49200472e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.91789054e-03 0.00000000e+00 6.29722401e-01 1.87809809e-03]\n",
      " [0.00000000e+00 0.00000000e+00 9.91081098e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13410095, -0.01721449,  1.41572402,  0.43696634]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1,env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[s,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
